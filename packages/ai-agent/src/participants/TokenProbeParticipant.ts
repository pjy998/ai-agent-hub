import * as vscode from 'vscode';
import { COPILOT_MODELS, CopilotModel } from '../features/token-probe';
import {
  createImprovedTokenProbe,
  ImprovedTokenProbeConfig,
  ImprovedTokenProbeResult,
} from '../features/improved-token-probe';
import { IntelligentParticipant, ExecutionFlow } from './base/intelligent-participant';
import { UserIntentAnalysis } from '../services/intelligent-input-analyzer';

/**
 * ç®€åŒ–çš„Tokenæ¢æµ‹é…ç½®
 */
interface SimpleTokenProbeConfig {
  model: CopilotModel;
  maxTokens: number;
  stepSize: number;
  timeout: number;
  retryCount: number;
}

/**
 * Token Probe Chat å‚ä¸è€…
 * ä½¿ç”¨GPT-4.1æ™ºèƒ½åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæä¾›Tokené™åˆ¶æµ‹è¯•åŠŸèƒ½
 */
export class TokenProbeParticipant extends IntelligentParticipant {
  constructor() {
    super('token-probe');
    this.initializeFlows();
  }

  /**
   * åˆå§‹åŒ–æ‰§è¡Œæµç¨‹
   */
  protected initializeFlows(): void {
    this.flows.set('token_probe', {
      name: 'Tokenæ¢æµ‹æµ‹è¯•',
      description: 'æ‰§è¡ŒTokené™åˆ¶æµ‹è¯•',
      supportedIntents: ['æµ‹è¯•', 'æ¢æµ‹', 'probe', 'test', 'æ£€æµ‹', 'é™åˆ¶'],
      execute: async (request, context, stream, token, analysis) => {
        await this.executeTokenProbeRequest(request.prompt, stream, token);
      },
    });

    this.flows.set('stats_report', {
      name: 'ç»Ÿè®¡æŠ¥å‘Š',
      description: 'æ˜¾ç¤ºTokenæµ‹è¯•ç»Ÿè®¡ä¿¡æ¯',
      supportedIntents: ['ç»Ÿè®¡', 'stats', 'æŠ¥å‘Š', 'report', 'åˆ†æ', 'analysis'],
      execute: async (request, context, stream, token, analysis) => {
        await this.executeStatsRequest(stream);
      },
    });

    this.flows.set('clear_history', {
      name: 'æ¸…é™¤å†å²',
      description: 'æ¸…é™¤Tokenæµ‹è¯•å†å²è®°å½•',
      supportedIntents: ['æ¸…é™¤', 'clear', 'åˆ é™¤', 'delete', 'é‡ç½®', 'reset'],
      execute: async (request, context, stream, token, analysis) => {
        await this.executeClearRequest(stream);
      },
    });

    this.flows.set('view_history', {
      name: 'æŸ¥çœ‹å†å²',
      description: 'æŸ¥çœ‹Tokenæµ‹è¯•å†å²è®°å½•',
      supportedIntents: ['å†å²', 'history', 'è®°å½•', 'record', 'ä¹‹å‰', 'previous'],
      execute: async (request, context, stream, token, analysis) => {
        await this.executeHistoryRequest(stream);
      },
    });

    this.flows.set('model_list', {
      name: 'æ¨¡å‹åˆ—è¡¨',
      description: 'æ˜¾ç¤ºæ”¯æŒçš„æ¨¡å‹åˆ—è¡¨',
      supportedIntents: ['æ¨¡å‹', 'model', 'åˆ—è¡¨', 'list', 'æ”¯æŒ', 'support'],
      execute: async (request, context, stream, token, analysis) => {
        await this.executeModelListRequest(stream);
      },
    });

    this.flows.set('help', {
      name: 'å¸®åŠ©ä¿¡æ¯',
      description: 'æ˜¾ç¤ºToken Probeä½¿ç”¨å¸®åŠ©',
      supportedIntents: ['å¸®åŠ©', 'help', 'ä½¿ç”¨', 'usage', 'å¦‚ä½•', 'how', 'æŒ‡å—', 'guide'],
      execute: async (request, context, stream, token, analysis) => {
        await this.executeHelpRequest(stream);
      },
    });

    this.flows.set('default', {
      name: 'é»˜è®¤å“åº”',
      description: 'æä¾›é»˜è®¤çš„Token Probeä»‹ç»',
      execute: async (request, context, stream, token, analysis) => {
        await this.executeDefaultRequest(stream);
      },
    });

    this.defaultFlow = 'default';
  }

  /**
   * æ£€æŸ¥å·¥ä½œåŒºçŠ¶æ€
   */
  protected async checkWorkspaceStatus(stream: vscode.ChatResponseStream): Promise<boolean> {
    if (!vscode.workspace.workspaceFolders || vscode.workspace.workspaceFolders.length === 0) {
      stream.markdown('âŒ **é”™è¯¯**: è¯·å…ˆæ‰“å¼€ä¸€ä¸ªå·¥ä½œåŒºæ‰èƒ½ä½¿ç”¨Token ProbeåŠŸèƒ½ã€‚');
      return false;
    }
    return true;
  }

  /**
   * æ‰§è¡ŒToken Probeæµ‹è¯•è¯·æ±‚
   */
  private async executeTokenProbeRequest(
    prompt: string,
    stream: vscode.ChatResponseStream,
    token: vscode.CancellationToken
  ): Promise<void> {
    if (!(await this.checkWorkspaceStatus(stream))) {
      return;
    }
    stream.markdown('ğŸš€ **å¼€å§‹Token Probeæµ‹è¯•**\n\næ­£åœ¨åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...');

    // è§£ææ¨¡å‹å‚æ•°
    const model = this.extractModelFromPrompt(prompt);
    const testMode = this.extractTestModeFromPrompt(prompt);

    // æ„å»ºé…ç½®
    const config = this.buildTestConfig(model, testMode);

    stream.markdown(
      `\nğŸ“‹ **æµ‹è¯•é…ç½®**:\n- æ¨¡å‹: ${COPILOT_MODELS[config.model].name}\n- æ¨¡å¼: ${testMode}\n- æœ€å¤§Token: ${config.maxTokens.toLocaleString()}\n`
    );

    try {
      // æ‰§è¡Œæµ‹è¯•
      stream.markdown('\nâ³ æ­£åœ¨æ‰§è¡ŒTokené™åˆ¶æµ‹è¯•ï¼Œè¯·ç¨å€™...');

      // åˆ›å»ºæµ‹è¯•æ–‡æœ¬
      const testText = prompt || 'è¿™æ˜¯ä¸€ä¸ªTokenæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºæ£€æµ‹æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚';

      // è½¬æ¢é…ç½®æ ¼å¼
      const improvedConfig: ImprovedTokenProbeConfig = {
        models: [
          {
            name: COPILOT_MODELS[config.model].name,
            maxTokens: config.maxTokens,
            costPer1kTokens: 0.002,
          },
        ],
        includeSystemPrompt: true,
        includeContext: true,
        outputFormat: 'detailed',
        showCosts: true,
      };

      const results = await createImprovedTokenProbe(improvedConfig, testText);
      const result = results[0]; // å–ç¬¬ä¸€ä¸ªç»“æœ

      // æ˜¾ç¤ºç»“æœ
      if (result.status === 'ok' || result.status === 'warning') {
        stream.markdown(`\nâœ… **æµ‹è¯•å®Œæˆï¼**\n\n`);
        stream.markdown(`ğŸ“Š **æµ‹è¯•ç»“æœ**:\n`);
        stream.markdown(`- **æ¨¡å‹**: ${result.model}\n`);
        stream.markdown(
          `- **Tokenä½¿ç”¨**: ${result.tokens.toLocaleString()}/${result.maxTokens.toLocaleString()} tokens\n`
        );
        stream.markdown(`- **ä½¿ç”¨ç‡**: ${result.utilization.toFixed(1)}%\n`);
        if (result.cost) {
          stream.markdown(`- **é¢„ä¼°æˆæœ¬**: $${result.cost.toFixed(4)}\n`);
        }

        // ä½¿ç”¨å»ºè®®
        stream.markdown(`\nğŸ’¡ **ä½¿ç”¨å»ºè®®**:\n`);
        if (result.utilization < 25) {
          stream.markdown('- é€‚åˆå¤§å‹é¡¹ç›®åˆ†æå’Œå¤æ‚ä»»åŠ¡\n');
        } else if (result.utilization < 75) {
          stream.markdown('- é€‚åˆä¸­ç­‰è§„æ¨¡é¡¹ç›®å’Œæ ‡å‡†ä»»åŠ¡\n');
        } else {
          stream.markdown('- é€‚åˆå°å‹é¡¹ç›®å’Œç®€å•æŸ¥è¯¢\n');
        }

        if (result.message) {
          stream.markdown(`\nâš ï¸ **æ³¨æ„**: ${result.message}\n`);
        }
      } else {
        stream.markdown(`\nâŒ **æµ‹è¯•å¤±è´¥**: ${result.message || 'æœªçŸ¥é”™è¯¯'}\n\n`);
        stream.markdown('ğŸ”§ **æ•…éšœæ’é™¤å»ºè®®**:\n');
        stream.markdown('1. æ£€æŸ¥ GitHub Copilot Chat æ‰©å±•æ˜¯å¦æ­£å¸¸å·¥ä½œ\n');
        stream.markdown('2. éªŒè¯ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š\n');
        stream.markdown('3. å°è¯•é‡æ–°å¯åŠ¨ VS Code\n');
      }
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      stream.markdown(`\nâŒ **æµ‹è¯•å¼‚å¸¸**: ${errorMessage}`);
    }
  }

  /**
   * æ‰§è¡Œç»Ÿè®¡æŠ¥å‘Šè¯·æ±‚
   */
  private async executeStatsRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('ğŸ“Š **Tokenä½¿ç”¨ç»Ÿè®¡**\n\n');
    stream.markdown('ğŸ’¡ **åŠŸèƒ½è¯´æ˜**:\n');
    stream.markdown('- ä½¿ç”¨ `@token æµ‹è¯•` å¼€å§‹Tokené™åˆ¶æµ‹è¯•\n');
    stream.markdown('- æ”¯æŒå¤šç§æ¨¡å‹çš„Tokenåˆ†æ\n');
    stream.markdown('- æä¾›è¯¦ç»†çš„ä½¿ç”¨ç‡å’Œæˆæœ¬ä¼°ç®—\n\n');

    stream.markdown('ğŸ“ˆ **æ”¯æŒçš„æ¨¡å‹**:\n');
    Object.entries(COPILOT_MODELS).forEach(([key, model]) => {
      stream.markdown(`- **${model.name}**: æœ€å¤§${model.maxTokens.toLocaleString()} tokens\n`);
    });

    stream.markdown('\nğŸš€ **å¼€å§‹æµ‹è¯•**: è¾“å…¥ `@token æµ‹è¯• gpt-4` æ¥æµ‹è¯•ç‰¹å®šæ¨¡å‹ï¼');
    stream.markdown('\nğŸ’¡ **æç¤º**: ä½¿ç”¨ `@token æ¸…é™¤` å¯ä»¥æ¸…ç©ºå†å²è®°å½•');
  }

  /**
   * æ‰§è¡Œæ¸…é™¤å†å²è¯·æ±‚
   */
  private async executeClearRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown(
      'ğŸ—‘ï¸ **æ¸…é™¤å†å²**: å†å²è®°å½•åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚\n\nğŸ’¡ ä½¿ç”¨ `@token æµ‹è¯•` å¼€å§‹æ–°çš„æµ‹è¯•ï¼'
    );
  }

  /**
   * æ‰§è¡Œå†å²è®°å½•è¯·æ±‚
   */
  private async executeHistoryRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown(
      'ğŸ“ **æµ‹è¯•å†å²**: å†å²è®°å½•åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚\n\nğŸ’¡ ä½¿ç”¨ `@token æµ‹è¯•` å¼€å§‹Token Probeæµ‹è¯•ï¼'
    );
  }

  /**
   * æ‰§è¡Œæ¨¡å‹åˆ—è¡¨è¯·æ±‚
   */
  private async executeModelListRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('ğŸ¤– **æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨**\n\n');

    // æŒ‰æä¾›å•†åˆ†ç»„
    const providers = new Map<string, Array<{ key: string; model: any }>>();

    Object.entries(COPILOT_MODELS).forEach(([key, model]) => {
      if (!providers.has(model.provider)) {
        providers.set(model.provider, []);
      }
      providers.get(model.provider)!.push({ key, model });
    });

    providers.forEach((models, provider) => {
      stream.markdown(`### ${provider}\n\n`);

      models.forEach(({ key, model }) => {
        const costIcon = model.costMultiplier === 0 ? 'ğŸ†“' : model.costMultiplier < 1 ? 'ğŸ’°' : 'ğŸ’¸';
        const previewBadge = model.isPreview ? ' `é¢„è§ˆç‰ˆ`' : '';
        const legacyBadge = model.isLegacy ? ' `æ—§ç‰ˆæœ¬`' : '';

        stream.markdown(`- ${costIcon} **${model.name}**${previewBadge}${legacyBadge}\n`);
        stream.markdown(`  ${model.description}\n`);
        stream.markdown(`  é¢„ä¼°ä¸Šä¸‹æ–‡: ${model.estimatedTokenLimit.toLocaleString()} tokens\n\n`);
      });
    });

    stream.markdown('ğŸ’¡ **ä½¿ç”¨ç¤ºä¾‹**: `@token æµ‹è¯• gpt-4.1` æˆ– `@token å¿«é€Ÿæµ‹è¯•`');
  }

  /**
   * æ‰§è¡Œå¸®åŠ©è¯·æ±‚
   */
  private async executeHelpRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('# ğŸ” Token Probe ä½¿ç”¨æŒ‡å—\n\n');

    stream.markdown('## ğŸ“– åŠŸèƒ½æ¦‚è¿°\n');
    stream.markdown(
      'Token Probe æ˜¯ä¸“ä¸º GitHub Copilot Chat è®¾è®¡çš„ä¸Šä¸‹æ–‡é™åˆ¶æµ‹è¯•å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æµ‹è¯•ä¸åŒæ¨¡å‹çš„æœ€å¤§Tokenä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n\n'
    );

    stream.markdown('## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n');
    stream.markdown('### åŸºæœ¬å‘½ä»¤\n');
    stream.markdown('- `@token æµ‹è¯•` - ä½¿ç”¨é»˜è®¤é…ç½®å¼€å§‹æµ‹è¯•\n');
    stream.markdown('- `@token å¿«é€Ÿæµ‹è¯•` - å¿«é€Ÿæµ‹è¯•æ¨¡å¼\n');
    stream.markdown('- `@token æµ‹è¯• gpt-4.1` - æµ‹è¯•æŒ‡å®šæ¨¡å‹\n');
    stream.markdown('- `@token å†å²` - æŸ¥çœ‹æµ‹è¯•å†å²\n');
    stream.markdown('- `@token ç»Ÿè®¡` - æŸ¥çœ‹è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š\n');
    stream.markdown('- `@token æ¨¡å‹` - æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨\n');
    stream.markdown('- `@token æ¸…é™¤` - æ¸…ç©ºæµ‹è¯•å†å²è®°å½•\n\n');

    stream.markdown('### æµ‹è¯•æ¨¡å¼\n');
    stream.markdown('- **å¿«é€Ÿæµ‹è¯•**: ä½¿ç”¨é¢„è®¾å‚æ•°ï¼Œé€‚åˆæ–°æ‰‹\n');
    stream.markdown('- **æ ‡å‡†æµ‹è¯•**: å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§\n');
    stream.markdown('- **æ·±åº¦æµ‹è¯•**: æœ€é«˜ç²¾åº¦ï¼Œè€—æ—¶è¾ƒé•¿\n\n');

    stream.markdown('## ğŸ’¡ ä½¿ç”¨æŠ€å·§\n');
    stream.markdown('1. é¦–æ¬¡ä½¿ç”¨å»ºè®®å…ˆè¿›è¡Œå¿«é€Ÿæµ‹è¯•\n');
    stream.markdown('2. å®šæœŸæµ‹è¯•äº†è§£æ¨¡å‹æ›´æ–°æƒ…å†µ\n');
    stream.markdown('3. æ ¹æ®é¡¹ç›®è§„æ¨¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n');
    stream.markdown('4. ä¿å­˜æµ‹è¯•å†å²ç”¨äºå¯¹æ¯”åˆ†æ\n\n');

    stream.markdown('ğŸ”§ å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·ä½¿ç”¨VS Codeå‘½ä»¤é¢æ¿ä¸­çš„ `AI Agent: Token Probe` å‘½ä»¤ã€‚');
  }

  /**
   * æ‰§è¡Œé»˜è®¤è¯·æ±‚
   */
  private async executeDefaultRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('ğŸ‘‹ **æ¬¢è¿ä½¿ç”¨ Token Probeï¼**\n\n');
    stream.markdown('æˆ‘å¯ä»¥å¸®åŠ©æ‚¨æµ‹è¯• GitHub Copilot Chat æ¨¡å‹çš„Tokenä¸Šä¸‹æ–‡é™åˆ¶ã€‚\n\n');

    stream.markdown('ğŸš€ **å¿«é€Ÿå¼€å§‹**:\n');
    stream.markdown('- è¾“å…¥ `æµ‹è¯•` å¼€å§‹Tokené™åˆ¶æµ‹è¯•\n');
    stream.markdown('- è¾“å…¥ `å†å²` æŸ¥çœ‹æµ‹è¯•è®°å½•\n');
    stream.markdown('- è¾“å…¥ `ç»Ÿè®¡` æŸ¥çœ‹è¯¦ç»†åˆ†ææŠ¥å‘Š\n');
    stream.markdown('- è¾“å…¥ `æ¨¡å‹` æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹\n');
    stream.markdown('- è¾“å…¥ `æ¸…é™¤` æ¸…ç©ºå†å²è®°å½•\n');
    stream.markdown('- è¾“å…¥ `å¸®åŠ©` è·å–è¯¦ç»†ä½¿ç”¨æŒ‡å—\n\n');

    stream.markdown('ğŸ’¡ **æç¤º**: Token Probe ä¼šè‡ªåŠ¨åˆ†æå½“å‰é¡¹ç›®å¹¶ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šï¼');
  }

  /**
   * ä»æç¤ºè¯ä¸­æå–æ¨¡å‹
   */
  private extractModelFromPrompt(prompt: string): CopilotModel {
    const modelKeys = Object.keys(COPILOT_MODELS) as CopilotModel[];

    for (const key of modelKeys) {
      const model = COPILOT_MODELS[key];
      if (prompt.includes(key) || prompt.includes(model.name.toLowerCase())) {
        return key;
      }
    }

    return 'gpt-4.1'; // é»˜è®¤æ¨¡å‹
  }

  /**
   * ä»æç¤ºè¯ä¸­æå–æµ‹è¯•æ¨¡å¼
   */
  private extractTestModeFromPrompt(prompt: string): string {
    if (prompt.includes('å¿«é€Ÿ') || prompt.includes('quick')) {
      return 'å¿«é€Ÿæµ‹è¯•';
    } else if (prompt.includes('æ·±åº¦') || prompt.includes('deep')) {
      return 'æ·±åº¦æµ‹è¯•';
    } else if (prompt.includes('è‡ªå®šä¹‰') || prompt.includes('custom')) {
      return 'è‡ªå®šä¹‰æµ‹è¯•';
    }
    return 'æ ‡å‡†æµ‹è¯•';
  }

  /**
   * æ„å»ºæµ‹è¯•é…ç½®
   */
  private buildTestConfig(model: CopilotModel, testMode: string): SimpleTokenProbeConfig {
    const baseConfig: SimpleTokenProbeConfig = {
      model,
      maxTokens: 200000,
      stepSize: 10000,
      timeout: 30000,
      retryCount: 3,
    };

    switch (testMode) {
      case 'å¿«é€Ÿæµ‹è¯•':
        return {
          ...baseConfig,
          maxTokens: 100000,
          stepSize: 20000,
          timeout: 15000,
          retryCount: 2,
        };
      case 'æ·±åº¦æµ‹è¯•':
        return {
          ...baseConfig,
          maxTokens: 300000,
          stepSize: 5000,
          timeout: 60000,
          retryCount: 5,
        };
      default:
        return baseConfig;
    }
  }

  /**
   * æ ¼å¼åŒ–æŒç»­æ—¶é—´
   */
  private formatDuration(ms: number): string {
    if (ms < 1000) {
      return `${ms}ms`;
    } else if (ms < 60000) {
      return `${(ms / 1000).toFixed(1)}s`;
    } else {
      const minutes = Math.floor(ms / 60000);
      const seconds = Math.floor((ms % 60000) / 1000);
      return `${minutes}m ${seconds}s`;
    }
  }
}
