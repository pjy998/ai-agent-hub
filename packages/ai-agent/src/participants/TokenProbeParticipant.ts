import * as vscode from 'vscode';
import { TokenProbeManager, COPILOT_MODELS, CopilotModel, TokenProbeConfig } from '../features/token-probe';

/**
 * Token Probe Chat å‚ä¸è€…
 * åœ¨ GitHub Copilot Chat ä¸­æä¾› Token é™åˆ¶æµ‹è¯•åŠŸèƒ½
 */
export class TokenProbeParticipant {
  private probeManager: TokenProbeManager;
  
  constructor() {
    this.probeManager = TokenProbeManager.getInstance();
  }
  
  /**
   * å¤„ç†èŠå¤©è¯·æ±‚
   */
  async handleRequest(
    request: vscode.ChatRequest,
    context: vscode.ChatContext,
    stream: vscode.ChatResponseStream,
    token: vscode.CancellationToken
  ): Promise<void> {
    const prompt = request.prompt.toLowerCase();
    
    try {
      // æ£€æŸ¥å·¥ä½œåŒº
      if (!vscode.workspace.workspaceFolders || vscode.workspace.workspaceFolders.length === 0) {
        stream.markdown('âŒ **é”™è¯¯**: è¯·å…ˆæ‰“å¼€ä¸€ä¸ªå·¥ä½œåŒºæ‰èƒ½ä½¿ç”¨Token ProbeåŠŸèƒ½ã€‚');
        return;
      }
      
      // è§£æç”¨æˆ·æ„å›¾
      if (this.isTokenProbeRequest(prompt)) {
        await this.handleTokenProbeRequest(prompt, stream, token);
      } else if (this.isHistoryRequest(prompt)) {
        await this.handleHistoryRequest(stream);
      } else if (this.isModelListRequest(prompt)) {
        await this.handleModelListRequest(stream);
      } else if (this.isHelpRequest(prompt)) {
        await this.handleHelpRequest(stream);
      } else {
        await this.handleDefaultRequest(stream);
      }
      
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      stream.markdown(`âŒ **Token Probe é”™è¯¯**: ${errorMessage}`);
    }
  }
  
  /**
   * åˆ¤æ–­æ˜¯å¦ä¸ºToken Probeæµ‹è¯•è¯·æ±‚
   */
  private isTokenProbeRequest(prompt: string): boolean {
    const keywords = ['æµ‹è¯•', 'test', 'probe', 'æ£€æµ‹', 'é™åˆ¶', 'limit', 'token', 'ä¸Šä¸‹æ–‡', 'context'];
    return keywords.some(keyword => prompt.includes(keyword));
  }
  
  /**
   * åˆ¤æ–­æ˜¯å¦ä¸ºå†å²è®°å½•è¯·æ±‚
   */
  private isHistoryRequest(prompt: string): boolean {
    const keywords = ['å†å²', 'history', 'è®°å½•', 'record', 'ä¹‹å‰', 'previous'];
    return keywords.some(keyword => prompt.includes(keyword));
  }
  
  /**
   * åˆ¤æ–­æ˜¯å¦ä¸ºæ¨¡å‹åˆ—è¡¨è¯·æ±‚
   */
  private isModelListRequest(prompt: string): boolean {
    const keywords = ['æ¨¡å‹', 'model', 'åˆ—è¡¨', 'list', 'æ”¯æŒ', 'support'];
    return keywords.some(keyword => prompt.includes(keyword));
  }
  
  /**
   * åˆ¤æ–­æ˜¯å¦ä¸ºå¸®åŠ©è¯·æ±‚
   */
  private isHelpRequest(prompt: string): boolean {
    const keywords = ['å¸®åŠ©', 'help', 'ä½¿ç”¨', 'usage', 'å¦‚ä½•', 'how', 'æŒ‡å—', 'guide'];
    return keywords.some(keyword => prompt.includes(keyword));
  }
  
  /**
   * å¤„ç†Token Probeæµ‹è¯•è¯·æ±‚
   */
  private async handleTokenProbeRequest(
    prompt: string,
    stream: vscode.ChatResponseStream,
    token: vscode.CancellationToken
  ): Promise<void> {
    stream.markdown('ğŸš€ **å¼€å§‹Token Probeæµ‹è¯•**\n\næ­£åœ¨åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...');
    
    // è§£ææ¨¡å‹å‚æ•°
    const model = this.extractModelFromPrompt(prompt);
    const testMode = this.extractTestModeFromPrompt(prompt);
    
    // æ„å»ºé…ç½®
    const config = this.buildTestConfig(model, testMode);
    
    stream.markdown(`\nğŸ“‹ **æµ‹è¯•é…ç½®**:\n- æ¨¡å‹: ${COPILOT_MODELS[config.model].name}\n- æ¨¡å¼: ${testMode}\n- æœ€å¤§Token: ${config.maxTokens.toLocaleString()}\n`);
    
    try {
      // æ‰§è¡Œæµ‹è¯•
      stream.markdown('\nâ³ æ­£åœ¨æ‰§è¡ŒTokené™åˆ¶æµ‹è¯•ï¼Œè¯·ç¨å€™...');
      
      const result = await this.probeManager.runProbe(config);
      
      // æ˜¾ç¤ºç»“æœ
      if (result.status === 'success') {
        stream.markdown(`\nâœ… **æµ‹è¯•å®Œæˆï¼**\n\n`);
        stream.markdown(`ğŸ“Š **æµ‹è¯•ç»“æœ**:\n`);
        stream.markdown(`- **æ¨¡å‹**: ${result.model}\n`);
        stream.markdown(`- **æœ€å¤§ä¸Šä¸‹æ–‡**: ${result.maxContextTokens.toLocaleString()} tokens\n`);
        stream.markdown(`- **æµ‹è¯•æ—¶é—´**: ${this.formatDuration(result.totalTestTime)}\n`);
        stream.markdown(`- **æµ‹è¯•æ­¥æ•°**: ${result.testSteps.length}\n`);
        
        // æ€§èƒ½åˆ†æ
        const avgResponseTime = result.testSteps.reduce((sum, step) => sum + step.responseTime, 0) / result.testSteps.length;
        stream.markdown(`\nğŸ“ˆ **æ€§èƒ½åˆ†æ**:\n`);
        stream.markdown(`- **å¹³å‡å“åº”æ—¶é—´**: ${avgResponseTime.toFixed(0)}ms\n`);
        stream.markdown(`- **æˆåŠŸç‡**: ${(result.testSteps.filter(s => s.result === 'success').length / result.testSteps.length * 100).toFixed(1)}%\n`);
        
        // ä½¿ç”¨å»ºè®®
        stream.markdown(`\nğŸ’¡ **ä½¿ç”¨å»ºè®®**:\n`);
        if (result.maxContextTokens > 100000) {
          stream.markdown('- é€‚åˆå¤§å‹é¡¹ç›®åˆ†æå’Œå¤æ‚ä»»åŠ¡\n');
        } else if (result.maxContextTokens > 50000) {
          stream.markdown('- é€‚åˆä¸­ç­‰è§„æ¨¡é¡¹ç›®å’Œæ ‡å‡†ä»»åŠ¡\n');
        } else {
          stream.markdown('- é€‚åˆå°å‹é¡¹ç›®å’Œç®€å•æŸ¥è¯¢\n');
        }
        
      } else {
        stream.markdown(`\nâŒ **æµ‹è¯•å¤±è´¥**: ${result.error}\n\n`);
        stream.markdown('ğŸ”§ **æ•…éšœæ’é™¤å»ºè®®**:\n');
        stream.markdown('1. æ£€æŸ¥ GitHub Copilot Chat æ‰©å±•æ˜¯å¦æ­£å¸¸å·¥ä½œ\n');
        stream.markdown('2. éªŒè¯ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š\n');
        stream.markdown('3. å°è¯•é‡æ–°å¯åŠ¨ VS Code\n');
      }
      
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      stream.markdown(`\nâŒ **æµ‹è¯•å¼‚å¸¸**: ${errorMessage}`);
    }
  }
  
  /**
   * å¤„ç†å†å²è®°å½•è¯·æ±‚
   */
  private async handleHistoryRequest(stream: vscode.ChatResponseStream): Promise<void> {
    const history = this.probeManager.getTestHistory();
    
    if (history.length === 0) {
      stream.markdown('ğŸ“ **æµ‹è¯•å†å²**: æš‚æ— æµ‹è¯•è®°å½•\n\nğŸ’¡ ä½¿ç”¨ `@token æµ‹è¯•` å¼€å§‹ç¬¬ä¸€æ¬¡Token Probeæµ‹è¯•ï¼');
      return;
    }
    
    stream.markdown('ğŸ“ **Token Probe æµ‹è¯•å†å²**\n\n');
    
    history.slice(-5).reverse().forEach((result, index) => {
      const statusIcon = result.status === 'success' ? 'âœ…' : 'âŒ';
      const date = new Date().toLocaleDateString(); // ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¿å­˜æµ‹è¯•æ—¶é—´
      
      stream.markdown(`${index + 1}. ${statusIcon} **${result.model}** - ${result.maxContextTokens.toLocaleString()} tokens\n`);
      stream.markdown(`   ğŸ“… ${date} | â±ï¸ ${this.formatDuration(result.totalTestTime)}\n\n`);
    });
    
    if (history.length > 5) {
      stream.markdown(`*æ˜¾ç¤ºæœ€è¿‘5æ¡è®°å½•ï¼Œå…±${history.length}æ¡*`);
    }
  }
  
  /**
   * å¤„ç†æ¨¡å‹åˆ—è¡¨è¯·æ±‚
   */
  private async handleModelListRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('ğŸ¤– **æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨**\n\n');
    
    // æŒ‰æä¾›å•†åˆ†ç»„
    const providers = new Map<string, Array<{key: string, model: any}>>;
    
    Object.entries(COPILOT_MODELS).forEach(([key, model]) => {
      if (!providers.has(model.provider)) {
        providers.set(model.provider, []);
      }
      providers.get(model.provider)!.push({key, model});
    });
    
    providers.forEach((models, provider) => {
      stream.markdown(`### ${provider}\n\n`);
      
      models.forEach(({key, model}) => {
        const costIcon = model.costMultiplier === 0 ? 'ğŸ†“' : model.costMultiplier < 1 ? 'ğŸ’°' : 'ğŸ’¸';
        const previewBadge = model.isPreview ? ' `é¢„è§ˆç‰ˆ`' : '';
        const legacyBadge = model.isLegacy ? ' `æ—§ç‰ˆæœ¬`' : '';
        
        stream.markdown(`- ${costIcon} **${model.name}**${previewBadge}${legacyBadge}\n`);
        stream.markdown(`  ${model.description}\n`);
        stream.markdown(`  é¢„ä¼°ä¸Šä¸‹æ–‡: ${model.estimatedTokenLimit.toLocaleString()} tokens\n\n`);
      });
    });
    
    stream.markdown('ğŸ’¡ **ä½¿ç”¨ç¤ºä¾‹**: `@token æµ‹è¯• gpt-4.1` æˆ– `@token å¿«é€Ÿæµ‹è¯•`');
  }
  
  /**
   * å¤„ç†å¸®åŠ©è¯·æ±‚
   */
  private async handleHelpRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('# ğŸ” Token Probe ä½¿ç”¨æŒ‡å—\n\n');
    
    stream.markdown('## ğŸ“– åŠŸèƒ½æ¦‚è¿°\n');
    stream.markdown('Token Probe æ˜¯ä¸“ä¸º GitHub Copilot Chat è®¾è®¡çš„ä¸Šä¸‹æ–‡é™åˆ¶æµ‹è¯•å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æµ‹è¯•ä¸åŒæ¨¡å‹çš„æœ€å¤§Tokenä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n\n');
    
    stream.markdown('## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n');
    stream.markdown('### åŸºæœ¬å‘½ä»¤\n');
    stream.markdown('- `@token æµ‹è¯•` - ä½¿ç”¨é»˜è®¤é…ç½®å¼€å§‹æµ‹è¯•\n');
    stream.markdown('- `@token å¿«é€Ÿæµ‹è¯•` - å¿«é€Ÿæµ‹è¯•æ¨¡å¼\n');
    stream.markdown('- `@token æµ‹è¯• gpt-4.1` - æµ‹è¯•æŒ‡å®šæ¨¡å‹\n');
    stream.markdown('- `@token å†å²` - æŸ¥çœ‹æµ‹è¯•å†å²\n');
    stream.markdown('- `@token æ¨¡å‹` - æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨\n\n');
    
    stream.markdown('### æµ‹è¯•æ¨¡å¼\n');
    stream.markdown('- **å¿«é€Ÿæµ‹è¯•**: ä½¿ç”¨é¢„è®¾å‚æ•°ï¼Œé€‚åˆæ–°æ‰‹\n');
    stream.markdown('- **æ ‡å‡†æµ‹è¯•**: å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§\n');
    stream.markdown('- **æ·±åº¦æµ‹è¯•**: æœ€é«˜ç²¾åº¦ï¼Œè€—æ—¶è¾ƒé•¿\n\n');
    
    stream.markdown('## ğŸ’¡ ä½¿ç”¨æŠ€å·§\n');
    stream.markdown('1. é¦–æ¬¡ä½¿ç”¨å»ºè®®å…ˆè¿›è¡Œå¿«é€Ÿæµ‹è¯•\n');
    stream.markdown('2. å®šæœŸæµ‹è¯•äº†è§£æ¨¡å‹æ›´æ–°æƒ…å†µ\n');
    stream.markdown('3. æ ¹æ®é¡¹ç›®è§„æ¨¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n');
    stream.markdown('4. ä¿å­˜æµ‹è¯•å†å²ç”¨äºå¯¹æ¯”åˆ†æ\n\n');
    
    stream.markdown('ğŸ”§ å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·ä½¿ç”¨VS Codeå‘½ä»¤é¢æ¿ä¸­çš„ `AI Agent: Token Probe` å‘½ä»¤ã€‚');
  }
  
  /**
   * å¤„ç†é»˜è®¤è¯·æ±‚
   */
  private async handleDefaultRequest(stream: vscode.ChatResponseStream): Promise<void> {
    stream.markdown('ğŸ‘‹ **æ¬¢è¿ä½¿ç”¨ Token Probeï¼**\n\n');
    stream.markdown('æˆ‘å¯ä»¥å¸®åŠ©æ‚¨æµ‹è¯• GitHub Copilot Chat æ¨¡å‹çš„Tokenä¸Šä¸‹æ–‡é™åˆ¶ã€‚\n\n');
    
    stream.markdown('ğŸš€ **å¿«é€Ÿå¼€å§‹**:\n');
    stream.markdown('- è¾“å…¥ `æµ‹è¯•` å¼€å§‹Tokené™åˆ¶æµ‹è¯•\n');
    stream.markdown('- è¾“å…¥ `å†å²` æŸ¥çœ‹æµ‹è¯•è®°å½•\n');
    stream.markdown('- è¾“å…¥ `æ¨¡å‹` æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹\n');
    stream.markdown('- è¾“å…¥ `å¸®åŠ©` è·å–è¯¦ç»†ä½¿ç”¨æŒ‡å—\n\n');
    
    stream.markdown('ğŸ’¡ **æç¤º**: Token Probe ä¼šè‡ªåŠ¨åˆ†æå½“å‰é¡¹ç›®å¹¶ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šï¼');
  }
  
  /**
   * ä»æç¤ºè¯ä¸­æå–æ¨¡å‹
   */
  private extractModelFromPrompt(prompt: string): CopilotModel {
    const modelKeys = Object.keys(COPILOT_MODELS) as CopilotModel[];
    
    for (const key of modelKeys) {
      const model = COPILOT_MODELS[key];
      if (prompt.includes(key) || prompt.includes(model.name.toLowerCase())) {
        return key;
      }
    }
    
    return 'gpt-4.1'; // é»˜è®¤æ¨¡å‹
  }
  
  /**
   * ä»æç¤ºè¯ä¸­æå–æµ‹è¯•æ¨¡å¼
   */
  private extractTestModeFromPrompt(prompt: string): string {
    if (prompt.includes('å¿«é€Ÿ') || prompt.includes('quick')) {
      return 'å¿«é€Ÿæµ‹è¯•';
    } else if (prompt.includes('æ·±åº¦') || prompt.includes('deep')) {
      return 'æ·±åº¦æµ‹è¯•';
    } else if (prompt.includes('è‡ªå®šä¹‰') || prompt.includes('custom')) {
      return 'è‡ªå®šä¹‰æµ‹è¯•';
    }
    return 'æ ‡å‡†æµ‹è¯•';
  }
  
  /**
   * æ„å»ºæµ‹è¯•é…ç½®
   */
  private buildTestConfig(model: CopilotModel, testMode: string): TokenProbeConfig {
    const baseConfig: TokenProbeConfig = {
      model,
      startTokens: 10000,
      maxTokens: 200000,
      stepSize: 10000,
      enableBinarySearch: true,
      timeout: 30000,
      retryCount: 3
    };
    
    switch (testMode) {
      case 'å¿«é€Ÿæµ‹è¯•':
        return {
          ...baseConfig,
          maxTokens: 100000,
          stepSize: 20000,
          timeout: 15000,
          retryCount: 2
        };
      case 'æ·±åº¦æµ‹è¯•':
        return {
          ...baseConfig,
          maxTokens: 300000,
          stepSize: 5000,
          timeout: 60000,
          retryCount: 5
        };
      default:
        return baseConfig;
    }
  }
  
  /**
   * æ ¼å¼åŒ–æŒç»­æ—¶é—´
   */
  private formatDuration(ms: number): string {
    if (ms < 1000) {
      return `${ms}ms`;
    } else if (ms < 60000) {
      return `${(ms / 1000).toFixed(1)}s`;
    } else {
      const minutes = Math.floor(ms / 60000);
      const seconds = Math.floor((ms % 60000) / 1000);
      return `${minutes}m ${seconds}s`;
    }
  }
}